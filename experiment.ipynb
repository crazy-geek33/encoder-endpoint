{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part, SafetySetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tushar.sharma\\AppData\\Local\\miniconda3\\envs\\lightrag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.service_account import Credentials\n",
    "from lightrag.base import BaseKVStorage\n",
    "\n",
    "# Create credentials object\n",
    "\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    filename='flowing-elf-441900-u8-fec05545b3e4.json',\n",
    "    scopes=['https://www.googleapis.com/auth/cloud-platform'])\n",
    "\n",
    "if credentials.expired:\n",
    "    credentials.refresh(Request())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide me with the list of popular baby names! I need the names to tell you anything about them. ðŸ˜Š \n",
      "\n",
      "I can then help you analyze the list, compare it to other years, or even tell you about the origins and meanings of the names.  \n"
     ]
    }
   ],
   "source": [
    "def generate():\n",
    "    vertexai.init(project=\"flowing-elf-441900-u8\", location=\"us-central1\", credentials=credentials)\n",
    "    model = GenerativeModel(\n",
    "        \"gemini-1.5-pro-001\",\n",
    "    )\n",
    "    responses = model.generate_content(\n",
    "        [\"The following is a list of the most popular baby names in the United States in 2021.\"],\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_settings,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    for response in responses:\n",
    "        print(response.text, end=\"\")\n",
    "\n",
    "\n",
    "generation_config = {\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
    "    ),\n",
    "]\n",
    "\n",
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project=\"flowing-elf-441900-u8\"\n",
    "location=\"us-central1\"\n",
    "credentials=credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Any\n",
    "from google.api_core import exceptions\n",
    "from vertexai.generative_models import (\n",
    "    GenerativeModel, \n",
    "    ChatSession,\n",
    "    Content\n",
    ")\n",
    "from vertexai.language_models import ChatModel\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "import vertexai\n",
    "from lightrag.base import BaseKVStorage\n",
    "from lightrag.utils import compute_args_hash, wrap_embedding_func_with_attrs\n",
    "\n",
    "class VertexAIError(Exception):\n",
    "    \"\"\"Generic error for issues related to Vertex AI.\"\"\"\n",
    "\n",
    "def init_vertexai():\n",
    "    \"\"\"Initialize Vertex AI with project details\"\"\"\n",
    "    vertexai.init(project=\"flowing-elf-441900-u8\", location=\"us-central1\", credentials=credentials)\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=10),\n",
    "    retry=retry_if_exception_type((\n",
    "        exceptions.ResourceExhausted,  # Rate limit\n",
    "        exceptions.ServiceUnavailable,  # Connection issues\n",
    "        exceptions.DeadlineExceeded,   # Timeout\n",
    "    ))\n",
    ")\n",
    "async def vertexai_complete_if_cache2(\n",
    "    model: str,\n",
    "    prompt: str,\n",
    "    system_prompt: Optional[str] = None,\n",
    "    history_messages: List[Dict[str, str]] = [],\n",
    "    **kwargs\n",
    ") -> str:\n",
    "    init_vertexai()\n",
    "\n",
    "    hashing_kv: BaseKVStorage = kwargs.pop(\"hashing_kv\", None)\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append(Content(role=\"system\", parts=system_prompt))\n",
    "    messages.extend([Content(role=msg[\"role\"], parts=msg[\"content\"]) for msg in history_messages])\n",
    "    messages.append(Content(role=\"user\", parts=prompt))\n",
    "\n",
    "    # Check cache if available\n",
    "    if hashing_kv is not None:\n",
    "        args_hash = compute_args_hash(model, messages)\n",
    "        cache_result = await hashing_kv.get_by_id(args_hash)\n",
    "        if cache_result is not None:\n",
    "            return cache_result[\"return\"]\n",
    "\n",
    "    # Initialize the model\n",
    "    generation_model = GenerativeModel(model)\n",
    "    chat: ChatSession = generation_model.start_chat()\n",
    "\n",
    "    # Add system prompt if provided\n",
    "    if system_prompt:\n",
    "        chat_params = kwargs.copy()\n",
    "        chat_params.pop(\"temperature\", None)  # Remove temperature if present\n",
    "        await chat.send_message_async(system_prompt, **chat_params)\n",
    "\n",
    "    # Add history messages\n",
    "    for message in history_messages:\n",
    "        chat_params = kwargs.copy()\n",
    "        chat_params.pop(\"temperature\", None)\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            # For assistant messages, we don't need to wait for response\n",
    "            chat._history.append(Content(role=\"assistant\", parts=message[\"content\"]))\n",
    "        else:\n",
    "            await chat.send_message_async(message[\"content\"], **chat_params)\n",
    "\n",
    "    # Send the actual prompt and get response\n",
    "    response = await chat.send_message_async(prompt, **kwargs)\n",
    "    response_text = response.text\n",
    "\n",
    "    # Cache the response if caching is enabled\n",
    "    if hashing_kv is not None:\n",
    "        await hashing_kv.upsert(\n",
    "            {args_hash: {\"return\": response_text, \"model\": model}}\n",
    "        )\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "import vertexai\n",
    "from google.api_core import exceptions\n",
    "from vertexai.preview.generative_models import GenerativeModel, ChatSession\n",
    "from typing import List, Dict, Optional, Any, Union\n",
    "from lightrag.base import BaseKVStorage\n",
    "from lightrag.utils import compute_args_hash, wrap_embedding_func_with_attrs\n",
    "\n",
    "def init_vertexai():\n",
    "    \"\"\"Initialize Vertex AI with project details\"\"\"\n",
    "    vertexai.init(project=\"flowing-elf-441900-u8\", location=\"us-central1\", credentials=credentials)\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=10),\n",
    "    retry=retry_if_exception_type((\n",
    "        exceptions.ResourceExhausted,  # Rate limit\n",
    "        exceptions.ServiceUnavailable,  # Connection issues\n",
    "        exceptions.DeadlineExceeded,   # Timeout\n",
    "    ))\n",
    ")\n",
    "async def vertexai_complete_if_cache(\n",
    "    model: str,\n",
    "    prompt: str,\n",
    "    system_prompt: Optional[str] = None,\n",
    "    history_messages: List[Dict[str, str]] = [],\n",
    "    **kwargs\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Async function to generate completions using Vertex AI Gemini with caching support\n",
    "    \n",
    "    Args:\n",
    "        model: Model name (e.g., \"gemini-pro\")\n",
    "        prompt: The user prompt\n",
    "        system_prompt: System prompt for context\n",
    "        history_messages: List of previous messages\n",
    "        project_id: Google Cloud project ID\n",
    "        location: Google Cloud location\n",
    "        hashing_kv: Cache storage interface\n",
    "        **kwargs: Additional parameters for model configuration\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated response content\n",
    "    \"\"\"\n",
    "    init_vertexai()\n",
    "    \n",
    "    hashing_kv: BaseKVStorage = kwargs.pop(\"hashing_kv\", None)\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.extend(history_messages)\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # Check cache if available\n",
    "    if hashing_kv is not None:\n",
    "        args_hash = compute_args_hash(model, messages)\n",
    "        cache_result = await hashing_kv.get_by_id(args_hash)\n",
    "        if cache_result is not None:\n",
    "            return cache_result[\"return\"]\n",
    "\n",
    "    # Initialize the model\n",
    "    generation_model = GenerativeModel(model)\n",
    "    chat: ChatSession = generation_model.start_chat()\n",
    "\n",
    "    # Add system prompt if provided\n",
    "    if system_prompt:\n",
    "        chat_params = kwargs.copy()\n",
    "        chat_params.pop(\"temperature\", None)  # Remove temperature if present\n",
    "        await chat.send_message_async(system_prompt, **chat_params)\n",
    "\n",
    "    # Add history messages\n",
    "    for message in history_messages:\n",
    "        chat_params = kwargs.copy()\n",
    "        chat_params.pop(\"temperature\", None)\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            # For assistant messages, we don't need to wait for response\n",
    "            await chat.send_message_async(message[\"content\"], **chat_params)\n",
    "        else:\n",
    "            await chat.send_message_async(message[\"content\"], **chat_params)\n",
    "\n",
    "    # Send the actual prompt and get response\n",
    "    response = await chat.send_message_async(prompt, **kwargs)\n",
    "    response_text = response.text\n",
    "\n",
    "    # Cache the response if caching is enabled\n",
    "    if hashing_kv is not None:\n",
    "        await hashing_kv.upsert(\n",
    "            {args_hash: {\"return\": response_text, \"model\": model}}\n",
    "        )\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def gemini_pro_complete(\n",
    "    prompt: str,\n",
    "    system_prompt: str = None,\n",
    "    history_messages: list = [],\n",
    "    **kwargs\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Helper function to generate completions using Gemini 1.5 Pro model.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The input prompt\n",
    "        system_prompt (str, optional): System prompt for setting context\n",
    "        history_messages (list, optional): List of previous conversation messages\n",
    "        project_id (str, optional): Google Cloud project ID\n",
    "        **kwargs: Additional parameters for model configuration\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated response from Gemini\n",
    "    \"\"\"\n",
    "    return await vertexai_complete_if_cache(\n",
    "        model=\"gemini-1.5-pro-001\",  # Gemini 1.5 Pro model identifier\n",
    "        prompt=prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        history_messages=history_messages,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alhamdulillah, I am doing well, thank you. ðŸ˜Š How are you? \\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await gemini_pro_complete(\"How are you?\", system_prompt=\"Respond with islamic greetings\", history_messages=[{\"role\": \"assistant\", \"content\": \"Assalamu Alaikum\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "from google.api_core import exceptions\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "import numpy as np\n",
    "\n",
    "@wrap_embedding_func_with_attrs(embedding_dim=768, max_token_size=3072)  # PaLM embedding dimensions\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=60),\n",
    "    retry=retry_if_exception_type((exceptions.ResourceExhausted, exceptions.ServiceUnavailable, exceptions.DeadlineExceeded))\n",
    ")\n",
    "async def vertexai_embedding(\n",
    "    texts: List[str],\n",
    "    model: str = \"textembedding-gecko@003\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using Vertex AI's embedding models.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of strings to generate embeddings for\n",
    "        model: Model name (e.g., \"textembedding-gecko@003\")\n",
    "        project_id: Google Cloud project ID\n",
    "        location: Google Cloud location\n",
    "        api_key: Google Cloud API key (optional)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Array of embeddings\n",
    "        \n",
    "    Raises:\n",
    "        VertexAIEmbeddingError: For any Vertex AI-specific errors\n",
    "    \"\"\"\n",
    "    # Set up environment variables if provided\n",
    "    init_vertexai()\n",
    "\n",
    "    # Initialize the embedding model\n",
    "    model = TextEmbeddingModel.from_pretrained(model)\n",
    "    response = model.get_embeddings(texts)\n",
    "\n",
    "    return np.array([emb.values for emb in response])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(await vertexai_embedding([\"Hello, how are you?\"])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightrag:Logger initialized for working directory: .\\merged\n",
      "INFO:lightrag:Load KV llm_response_cache with 1 data\n",
      "INFO:lightrag:Load KV full_docs with 0 data\n",
      "INFO:lightrag:Load KV text_chunks with 0 data\n",
      "INFO:lightrag:Loaded graph from .\\merged\\graph_chunk_entity_relation.graphml with 0 nodes, 0 edges\n",
      "INFO:nano-vectordb:Load (0, 768) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '.\\\\merged\\\\vdb_entities.json'} 0 data\n",
      "INFO:nano-vectordb:Load (0, 768) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '.\\\\merged\\\\vdb_relationships.json'} 0 data\n",
      "INFO:nano-vectordb:Load (1, 768) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '.\\\\merged\\\\vdb_chunks.json'} 1 data\n"
     ]
    }
   ],
   "source": [
    "from lightrag.lightrag import LightRAG, EmbeddingFunc, QueryParam\n",
    "\n",
    "rag = LightRAG(\n",
    "    working_dir=r'.\\merged',\n",
    "    llm_model_func=gemini_pro_complete,\n",
    "    embedding_func=EmbeddingFunc(\n",
    "        embedding_dim=768,\n",
    "        max_token_size=8192,\n",
    "        func=vertexai_embedding,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightrag:[New Docs] inserting 1 docs\n",
      "INFO:lightrag:[New Chunks] inserting 1 chunks\n",
      "INFO:lightrag:Inserting 1 vectors to chunks\n",
      "INFO:lightrag:[Entity Extraction]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ™ Processed 1 chunks, 19 entities(duplicated), 18 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightrag:Inserting 19 vectors to entities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightrag:Inserting 18 vectors to relationships\n",
      "INFO:lightrag:Writing graph with 19 nodes, 18 edges\n"
     ]
    }
   ],
   "source": [
    "with open(\"./android_versions_detailed.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    await rag.ainsert(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightrag:Local query uses 19 entites, 18 relations, 1 text units\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Android 3.0, codenamed Honeycomb, was designed specifically for tablets. \\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightrag.lightrag import QueryParam\n",
    "\n",
    "await rag.aquery(\"Which android version is designed specifically for tablets ?\", param=QueryParam(mode=\"local\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
